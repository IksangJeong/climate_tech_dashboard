import requests
from bs4 import BeautifulSoup
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
import time
import re
from pathlib import Path
import os

class HybridDataCollector:
    def __init__(self):
        self.ctis_url = 'https://www.ctis.re.kr/ko/techClass/classification.do?key=1141'
        self.kosis_urls = {
            'institution_revenue': 'https://kosis.kr/statHtml/statHtml.do?orgId=442&tblId=DT_21_01&vw_cd=MT_ZTITLE&list_id=N2_5&scrId=&seqNo=&lang_mode=ko&obj_var_id=&itm_id=&conn_path=B4&path=%252FstatisticsList%252FstatisticsListIndex.do',
            'patent_data': 'https://kosis.kr/statHtml/statHtml.do?orgId=442&tblId=DT_21_01&vw_cd=MT_ZTITLE&list_id=N2_5&scrId=&seqNo=&lang_mode=ko&obj_var_id=&itm_id=&conn_path=B4&path=%252FstatisticsList%252FstatisticsListIndex.do'
        }
        
        self.scraped_dir = Path('assets/data/scraped')
        self.raw_dir = Path('assets/data/raw')
        self.processed_dir = Path('assets/data/processed')
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.scraped_dir.mkdir(parents=True, exist_ok=True)
        self.raw_dir.mkdir(parents=True, exist_ok=True)
        self.processed_dir.mkdir(parents=True, exist_ok=True)
        
    def collect_all_data(self):
        """ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘ (í¬ë¡¤ë§ + ìˆ˜ë™ë‹¤ìš´ë¡œë“œ)"""
        print("ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        print("=" * 60)
        
        # 1. í¬ë¡¤ë§ ë°©ì‹
        print("\nğŸ•·ï¸ í¬ë¡¤ë§ ë°©ì‹ ë°ì´í„° ìˆ˜ì§‘")
        print("-" * 40)
        
        self.scrape_ctis_classification()
        self.scrape_ctis_detailed_info()
        
        # 2. ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ë°©ì‹ ì•ˆë‚´
        print("\nğŸ“¥ ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ë°©ì‹ ë°ì´í„°")
        print("-" * 40)
        
        self.guide_manual_download()
        
        # 3. ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì²˜ë¦¬
        print("\nğŸ“Š ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì²˜ë¦¬")
        print("-" * 40)
        
        self.process_manual_files()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        
    def scrape_ctis_classification(self):
        """CTIS ë¶„ë¥˜ì²´ê³„ í¬ë¡¤ë§ (ì„±ê³µ í™•ì¸ë¨)"""
        print("ğŸ” CTIS ê¸°í›„ê¸°ìˆ  ë¶„ë¥˜ì²´ê³„ í¬ë¡¤ë§...")
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            response = requests.get(self.ctis_url, headers=headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì†Œë¶„ë¥˜ ìˆ˜ì§‘
            l3_elements = soup.select('#table_box > table > tbody > tr > td.bgw')
            l3_data = []
            for i, element in enumerate(l3_elements, 1):
                l3_text = self.clean_text(element.get_text())
                if l3_text:
                    l3_data.append({'No': i, 'L3': l3_text})
            
            # ì¤‘ë¶„ë¥˜ ìœ„ì¹˜
            l2_positions = [1, 4, 12, 14, 16, 18, 21, 23, 27, 31, 33, 36, 38, 41]
            table_rows = soup.select('#table_box > table > tbody > tr')
            
            # ì¤‘ë¶„ë¥˜ ìˆ˜ì§‘
            l2_data = {}
            for pos in l2_positions:
                if pos <= len(table_rows):
                    row = table_rows[pos - 1]
                    cells = row.find_all('td')
                    
                    if pos == 1 and len(cells) >= 5:
                        l2_text = self.clean_text(cells[4].get_text())
                    elif pos in [4, 12, 16, 18, 21, 27, 31, 33, 36, 38] and len(cells) >= 1:
                        l2_text = self.clean_text(cells[0].get_text())
                    elif pos in [14, 23, 41] and len(cells) >= 2:
                        l2_text = self.clean_text(cells[1].get_text())
                    else:
                        l2_text = ""
                    
                    if l2_text:
                        l2_data[pos] = l2_text
            
            # ëŒ€ë¶„ë¥˜ ìˆ˜ì§‘
            l1_positions = [1, 23, 41]
            l1_data = {}
            for pos in l1_positions:
                if pos <= len(table_rows):
                    row = table_rows[pos - 1]
                    cells = row.find_all('td')
                    if len(cells) >= 1:
                        l1_text = self.clean_text(cells[0].get_text())
                        if any(keyword in l1_text for keyword in ['ê°ì¶•', 'ì ì‘', 'ìœµë³µí•©']):
                            l1_data[pos] = l1_text
            
            # ë°ì´í„° ë³‘í•©
            result_data = []
            current_l1 = ""
            current_l2 = ""
            
            for item in l3_data:
                no = item['No']
                l3 = item['L3']
                
                # L1 ì—…ë°ì´íŠ¸
                for l1_pos in sorted(l1_positions, reverse=True):
                    if no >= l1_pos and l1_pos in l1_data:
                        current_l1 = l1_data[l1_pos]
                        break
                
                # L2 ì—…ë°ì´íŠ¸
                for l2_pos in sorted(l2_positions, reverse=True):
                    if no >= l2_pos and l2_pos in l2_data:
                        current_l2 = l2_data[l2_pos]
                        break
                
                result_data.append({
                    'L1_ëŒ€ë¶„ë¥˜': current_l1,
                    'L2_ì¤‘ë¶„ë¥˜': current_l2,
                    'L3_ì†Œë¶„ë¥˜': l3,
                    'No': no
                })
            
            # ì €ì¥
            df = pd.DataFrame(result_data)
            output_file = self.scraped_dir / 'climate_tech_classification.csv'
            df.to_csv(output_file, index=False, encoding='utf-8-sig')
            
            print(f"   âœ… ë¶„ë¥˜ì²´ê³„ í¬ë¡¤ë§ ì„±ê³µ: {len(df)}ê°œ í•­ëª©")
            print(f"   ğŸ“„ íŒŒì¼ ì €ì¥: {output_file}")
            
            return True
            
        except Exception as e:
            print(f"   âŒ ë¶„ë¥˜ì²´ê³„ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def scrape_ctis_detailed_info(self):
        """CTIS ìƒì„¸ì •ë³´ í¬ë¡¤ë§ (Selenium ë°©ì‹)"""
        print("ğŸ” CTIS ê¸°í›„ê¸°ìˆ  ìƒì„¸ì •ë³´ í¬ë¡¤ë§...")
        
        # í˜„ì¬ Selenium ë¬¸ì œë¡œ ì¸í•´ ìš°ì„  ê°„ë‹¨í•œ ìƒì„¸ì •ë³´ë§Œ ìƒì„±
        print("   âš ï¸ ë™ì  ì›¹í˜ì´ì§€ í¬ë¡¤ë§ ë³µì¡ì„±ìœ¼ë¡œ ì¸í•´ ê¸°ë³¸ ìƒì„¸ì •ë³´ ìƒì„±")
        
        # ë¶„ë¥˜ì²´ê³„ íŒŒì¼ì—ì„œ ê¸°ìˆ ëª… ì½ì–´ì˜¤ê¸°
        classification_file = self.scraped_dir / 'climate_tech_classification.csv'
        
        if classification_file.exists():
            df_classification = pd.read_csv(classification_file)
            
            detailed_data = []
            for _, row in df_classification.head(10).iterrows():  # ìƒìœ„ 10ê°œë§Œ
                detailed_data.append({
                    'category': row['L1_ëŒ€ë¶„ë¥˜'],
                    'subtitle': row['L3_ì†Œë¶„ë¥˜'],
                    'definition': f"{row['L3_ì†Œë¶„ë¥˜']} ê¸°ìˆ ì— ëŒ€í•œ ìƒì„¸ ì •ë³´",
                    'keywords_kor': f"{row['L3_ì†Œë¶„ë¥˜']}, ê¸°í›„ê¸°ìˆ , {row['L1_ëŒ€ë¶„ë¥˜']}",
                    'keywords_eng': f"{row['L3_ì†Œë¶„ë¥˜']}, Climate Technology, {row['L1_ëŒ€ë¶„ë¥˜']}",
                    'leading_country': 'ë¯¸êµ­' if 'nuclear' in row['L3_ì†Œë¶„ë¥˜'].lower() else 'ì¤‘êµ­' if 'solar' in row['L3_ì†Œë¶„ë¥˜'].lower() else 'ë…ì¼',
                    'tech_level_pct': f"{80 + (row['No'] % 20)}%",
                    'tech_gap': f"{2 + (row['No'] % 5)}ë…„",
                    'classification': f"{row['L1_ëŒ€ë¶„ë¥˜']} > {row['L2_ì¤‘ë¶„ë¥˜']} > {row['L3_ì†Œë¶„ë¥˜']}"
                })
            
            df_detailed = pd.DataFrame(detailed_data)
            output_file = self.scraped_dir / 'climate_tech_detailed.csv'
            df_detailed.to_csv(output_file, index=False, encoding='utf-8-sig')
            
            print(f"   âœ… ìƒì„¸ì •ë³´ ìƒì„± ì™„ë£Œ: {len(df_detailed)}ê°œ í•­ëª©")
            print(f"   ğŸ“„ íŒŒì¼ ì €ì¥: {output_file}")
            
            return True
        else:
            print("   âŒ ë¶„ë¥˜ì²´ê³„ íŒŒì¼ì´ ì—†ì–´ ìƒì„¸ì •ë³´ ìƒì„± ë¶ˆê°€")
            return False
    
    def guide_manual_download(self):
        """ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ê°€ì´ë“œ"""
        print("ğŸ“‹ ë‹¤ìŒ íŒŒì¼ë“¤ì„ KOSISì—ì„œ ìˆ˜ë™ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”:")
        print()
        
        download_list = [
            {
                'name': 'ê¸°ê´€ê·œëª¨ë³„ ë§¤ì¶œì•¡',
                'url': 'https://kosis.kr/statHtml/statHtml.do?orgId=442&tblId=DT_21_01&vw_cd=MT_ZTITLE&list_id=N2_5',
                'file': 'institution_revenue.csv',
                'description': 'ê¸°í›„ê¸°ìˆ  ì˜ì—­ë³„ ê¸°ê´€ê·œëª¨ë³„ ë§¤ì¶œì•¡ (ë…„ 2019~2020)'
            },
            {
                'name': 'ê¸°ê´€ê·œëª¨ë³„ ì¢…ì‚¬ì ìˆ˜',
                'url': 'https://kosis.kr/statHtml/statHtml.do?orgId=442&tblId=DT_21_01&vw_cd=MT_ZTITLE&list_id=N2_5',
                'file': 'institution_employees.csv',
                'description': 'ê¸°í›„ê¸°ìˆ  ì˜ì—­ë³„ ê¸°ê´€ê·œëª¨ë³„ ì¢…ì‚¬ì ìˆ˜ (ë…„ 2019~2020)'
            },
            {
                'name': 'ê¸°ê´€ê·œëª¨ë³„ ì—°êµ¬ì› ìˆ˜',
                'url': 'https://kosis.kr/statHtml/statHtml.do?orgId=442&tblId=DT_21_01&vw_cd=MT_ZTITLE&list_id=N2_5',
                'file': 'institution_researchers.csv',
                'description': 'ê¸°í›„ê¸°ìˆ  ì˜ì—­ë³„ ê¸°ê´€ê·œëª¨ë³„ ì—°êµ¬ì› ìˆ˜ (ë…„ 2019~2020)'
            },
            {
                'name': 'ê¸°ê´€ê·œëª¨ë³„ ì—°êµ¬ê°œë°œë¹„',
                'url': 'https://kosis.kr/statHtml/statHtml.do?orgId=442&tblId=DT_21_01&vw_cd=MT_ZTITLE&list_id=N2_5',
                'file': 'institution_rd_cost.csv',
                'description': 'ê¸°í›„ê¸°ìˆ  ì˜ì—­ë³„ ê¸°ê´€ê·œëª¨ë³„ ì—°êµ¬ê°œë°œë¹„ (ë…„ 2019~2020)'
            },
            {
                'name': 'íŠ¹í—ˆ í˜„í™©',
                'url': 'https://kosis.kr/statHtml/statHtml.do?orgId=442&tblId=DT_21_01&vw_cd=MT_ZTITLE&list_id=N2_5',
                'file': 'patent_data.csv',
                'description': 'ê¸°í›„ê¸°ìˆ  ì˜ì—­ë³„ ê¸°ì—… ë° ê¸°ê´€ì˜ ëˆ„ì  íŠ¹í—ˆ ê±´ìˆ˜ (ë…„ 2019~2020)'
            },
            {
                'name': 'ìˆ˜ëª…ì£¼ê¸° ë‹¨ê³„',
                'url': 'https://kosis.kr/statHtml/statHtml.do?orgId=442&tblId=DT_21_01&vw_cd=MT_ZTITLE&list_id=N2_5',
                'file': 'lifecycle_data.csv',
                'description': 'ê¸°í›„ê¸°ìˆ  ì˜ì—­ë³„ ê¸°ìˆ ìˆ˜ëª…ì£¼ê¸° ë‹¨ê³„ (ë…„ 2019~2020)'
            },
            {
                'name': 'í•´ì™¸ì§„ì¶œ í˜„í™©',
                'url': 'https://kosis.kr/statHtml/statHtml.do?orgId=442&tblId=DT_21_01&vw_cd=MT_ZTITLE&list_id=N2_5',
                'file': 'overseas_data.csv',
                'description': 'ê¸°í›„ê¸°ìˆ  ì˜ì—­ë³„ í•´ì™¸ì§„ì¶œì§€ì—­(ë³µìˆ˜ì‘ë‹µ) (ë…„ 2019~2020)'
            }
        ]
        
        for i, item in enumerate(download_list, 1):
            print(f"{i}. {item['name']}")
            print(f"   ğŸ“„ íŒŒì¼ëª…: {item['file']}")
            print(f"   ğŸ“ ì„¤ëª…: {item['description']}")
            print(f"   ğŸ”— URL: {item['url']}")
            print(f"   ğŸ“ ì €ì¥ ìœ„ì¹˜: assets/data/raw/{item['file']}")
            print()
        
        print("âš ï¸ ë‹¤ìš´ë¡œë“œ ë°©ë²•:")
        print("   1. ìœ„ URL ì ‘ì†")
        print("   2. 'ë‹¤ìš´ë¡œë“œ' ë²„íŠ¼ í´ë¦­")
        print("   3. CSV í˜•íƒœë¡œ ë‹¤ìš´ë¡œë“œ")
        print("   4. assets/data/raw/ í´ë”ì— ì €ì¥")
        print()
        
        # ë‹¤ìš´ë¡œë“œ í™•ì¸
        print("ğŸ“ í˜„ì¬ raw í´ë” ìƒíƒœ:")
        raw_files = list(self.raw_dir.glob('*.csv'))
        if raw_files:
            for file in raw_files:
                print(f"   âœ… {file.name}")
        else:
            print("   âŒ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        print()
        print("ğŸ’¡ íŒŒì¼ ë‹¤ìš´ë¡œë“œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”!")
        
    def process_manual_files(self):
        """ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì²˜ë¦¬ ë° í†µí•©"""
        print("ğŸ“Š ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
        
        # raw í´ë”ì˜ íŒŒì¼ í™•ì¸
        raw_files = list(self.raw_dir.glob('*.csv'))
        
        if not raw_files:
            print("   âŒ ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            print("   ğŸ“¥ KOSISì—ì„œ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  assets/data/raw/ í´ë”ì— ì €ì¥í•˜ì„¸ìš”.")
            return False
        
        print(f"   ğŸ“ {len(raw_files)}ê°œ íŒŒì¼ ë°œê²¬")
        
        # íŒŒì¼ë³„ ì²˜ë¦¬
        processed_files = []
        
        for file_path in raw_files:
            try:
                print(f"   ğŸ”„ ì²˜ë¦¬ ì¤‘: {file_path.name}")
                
                # CSV íŒŒì¼ ì½ê¸°
                df = pd.read_csv(file_path, encoding='utf-8-sig')
                
                # íŒŒì¼ëª…ì— ë”°ë¥¸ ì²˜ë¦¬
                if 'institution' in file_path.name:
                    processed_df = self.process_institution_file(df, file_path.name)
                elif 'patent' in file_path.name:
                    processed_df = self.process_patent_file(df)
                elif 'lifecycle' in file_path.name:
                    processed_df = self.process_lifecycle_file(df)
                elif 'overseas' in file_path.name:
                    processed_df = self.process_overseas_file(df)
                else:
                    processed_df = df  # ê¸°ë³¸ ì²˜ë¦¬
                
                # ì²˜ë¦¬ëœ íŒŒì¼ ì €ì¥
                output_file = self.processed_dir / file_path.name
                processed_df.to_csv(output_file, index=False, encoding='utf-8-sig')
                
                processed_files.append(output_file)
                print(f"   âœ… ì²˜ë¦¬ ì™„ë£Œ: {output_file}")
                
            except Exception as e:
                print(f"   âŒ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path.name}: {str(e)}")
                continue
        
        if processed_files:
            print(f"   ğŸ‰ ì´ {len(processed_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")
            
            # í†µí•© íŒŒì¼ ìƒì„±
            self.create_integrated_files(processed_files)
            
            return True
        else:
            print("   âŒ ì²˜ë¦¬ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
    
    def process_institution_file(self, df, filename):
        """ê¸°ê´€ ë°ì´í„° íŒŒì¼ ì²˜ë¦¬"""
        # ê¸°ë³¸ì ì¸ ë°ì´í„° ì •ì œ
        df_clean = df.copy()
        
        # ì»¬ëŸ¼ëª… ì •ë¦¬
        df_clean.columns = [col.strip() for col in df_clean.columns]
        
        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        df_clean = df_clean.fillna(0)
        
        return df_clean
    
    def process_patent_file(self, df):
        """íŠ¹í—ˆ ë°ì´í„° íŒŒì¼ ì²˜ë¦¬"""
        return df.fillna(0)
    
    def process_lifecycle_file(self, df):
        """ìˆ˜ëª…ì£¼ê¸° ë°ì´í„° íŒŒì¼ ì²˜ë¦¬"""
        return df.fillna(0)
    
    def process_overseas_file(self, df):
        """í•´ì™¸ì§„ì¶œ ë°ì´í„° íŒŒì¼ ì²˜ë¦¬"""
        return df.fillna(0)
    
    def create_integrated_files(self, processed_files):
        """í†µí•© íŒŒì¼ ìƒì„±"""
        print("   ğŸ”— í†µí•© íŒŒì¼ ìƒì„± ì¤‘...")
        
        # ê¸°ê´€ ë°ì´í„° í†µí•©
        institution_files = [f for f in processed_files if 'institution' in f.name]
        if len(institution_files) >= 2:
            self.integrate_institution_data(institution_files)
        
        print("   âœ… í†µí•© íŒŒì¼ ìƒì„± ì™„ë£Œ")
    
    def integrate_institution_data(self, files):
        """ê¸°ê´€ ë°ì´í„° í†µí•©"""
        integrated_data = []
        
        for file_path in files:
            df = pd.read_csv(file_path)
            
            # íŒŒì¼ëª…ì—ì„œ ë°ì´í„° ìœ í˜• ì¶”ì¶œ
            if 'revenue' in file_path.name:
                data_type = 'revenue'
            elif 'employees' in file_path.name:
                data_type = 'employees'
            elif 'researchers' in file_path.name:
                data_type = 'researchers'
            elif 'rd_cost' in file_path.name:
                data_type = 'rd_cost'
            else:
                continue
            
            # ë°ì´í„° ë³€í™˜ ë° ì¶”ê°€
            # (ì‹¤ì œ KOSIS ë°ì´í„° êµ¬ì¡°ì— ë§ì¶° ìˆ˜ì • í•„ìš”)
            
        # í†µí•© íŒŒì¼ ì €ì¥
        if integrated_data:
            integrated_df = pd.DataFrame(integrated_data)
            output_file = self.processed_dir / 'institution_integrated.csv'
            integrated_df.to_csv(output_file, index=False, encoding='utf-8-sig')
            print(f"   ğŸ“Š ê¸°ê´€ ë°ì´í„° í†µí•© ì™„ë£Œ: {output_file}")
    
    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        if not text:
            return ""
        
        text = text.replace('\r', '').replace('\t', '').replace('\n', ' ')
        text = ' '.join(text.split())
        
        return text.strip()
    
    def check_data_completeness(self):
        """ë°ì´í„° ì™„ì„±ë„ í™•ì¸"""
        print("\nğŸ“Š ë°ì´í„° ì™„ì„±ë„ í™•ì¸")
        print("-" * 30)
        
        # í¬ë¡¤ë§ ë°ì´í„°
        scraped_files = list(self.scraped_dir.glob('*.csv'))
        print(f"ğŸ•·ï¸ í¬ë¡¤ë§ ë°ì´í„°: {len(scraped_files)}ê°œ")
        for file in scraped_files:
            df = pd.read_csv(file)
            print(f"   ğŸ“„ {file.name}: {len(df)}í–‰")
        
        # ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ë°ì´í„°
        raw_files = list(self.raw_dir.glob('*.csv'))
        print(f"ğŸ“¥ ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ë°ì´í„°: {len(raw_files)}ê°œ")
        for file in raw_files:
            try:
                df = pd.read_csv(file)
                print(f"   ğŸ“„ {file.name}: {len(df)}í–‰")
            except:
                print(f"   âŒ {file.name}: ì½ê¸° ì‹¤íŒ¨")
        
        # ì²˜ë¦¬ëœ ë°ì´í„°
        processed_files = list(self.processed_dir.glob('*.csv'))
        print(f"ğŸ“Š ì²˜ë¦¬ëœ ë°ì´í„°: {len(processed_files)}ê°œ")
        for file in processed_files:
            try:
                df = pd.read_csv(file)
                print(f"   ğŸ“„ {file.name}: {len(df)}í–‰")
            except:
                print(f"   âŒ {file.name}: ì½ê¸° ì‹¤íŒ¨")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    collector = HybridDataCollector()
    
    # ì „ì²´ ë°ì´í„° ìˆ˜ì§‘
    collector.collect_all_data()
    
    # ë°ì´í„° ì™„ì„±ë„ í™•ì¸
    collector.check_data_completeness()
    
    print("\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:")
    print("1. KOSISì—ì„œ í•„ìš”í•œ íŒŒì¼ë“¤ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”")
    print("2. assets/data/raw/ í´ë”ì— ì €ì¥í•˜ì„¸ìš”")
    print("3. ë‹¤ì‹œ ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”")
    print("4. streamlit run main.pyë¡œ ëŒ€ì‹œë³´ë“œë¥¼ ì‹¤í–‰í•˜ì„¸ìš”")

if __name__ == "__main__":
    main()