"""
í—¬í¼ í•¨ìˆ˜ë“¤
"""

import pandas as pd
import numpy as np
import streamlit as st
from pathlib import Path
import os
import json
from datetime import datetime, timedelta

def load_data_with_cache(file_path, cache_key, create_sample_func=None):
    """ìºì‹±ëœ ë°ì´í„° ë¡œë“œ"""
    @st.cache_data
    def _load_data():
        try:
            if Path(file_path).exists():
                return pd.read_csv(file_path)
            elif create_sample_func:
                return create_sample_func()
            else:
                return pd.DataFrame()
        except Exception as e:
            st.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            if create_sample_func:
                return create_sample_func()
            return pd.DataFrame()
    
    return _load_data()

def safe_divide(numerator, denominator):
    """ì•ˆì „í•œ ë‚˜ëˆ—ì…ˆ"""
    if denominator == 0 or pd.isna(denominator):
        return 0
    return numerator / denominator

def calculate_growth_rate(current_value, previous_value):
    """ì„±ì¥ë¥  ê³„ì‚°"""
    if previous_value == 0 or pd.isna(previous_value):
        return 0
    return ((current_value - previous_value) / previous_value) * 100

def format_large_number(number):
    """í° ìˆ«ì í¬ë§·íŒ…"""
    if pd.isna(number):
        return "N/A"
    
    if number >= 1_000_000_000:
        return f"{number/1_000_000_000:.1f}B"
    elif number >= 1_000_000:
        return f"{number/1_000_000:.1f}M"
    elif number >= 1_000:
        return f"{number/1_000:.1f}K"
    else:
        return f"{number:.0f}"

def create_download_link(df, filename, link_text="ë‹¤ìš´ë¡œë“œ"):
    """ë°ì´í„°í”„ë ˆì„ ë‹¤ìš´ë¡œë“œ ë§í¬ ìƒì„±"""
    csv = df.to_csv(index=False, encoding='utf-8-sig')
    return st.download_button(
        label=f"ğŸ“¥ {link_text}",
        data=csv,
        file_name=filename,
        mime="text/csv"
    )

def filter_dataframe(df, filters):
    """ë°ì´í„°í”„ë ˆì„ í•„í„°ë§"""
    filtered_df = df.copy()
    
    for column, value in filters.items():
        if value != "ì „ì²´" and value is not None:
            if isinstance(value, list):
                filtered_df = filtered_df[filtered_df[column].isin(value)]
            else:
                filtered_df = filtered_df[filtered_df[column] == value]
    
    return filtered_df

def get_top_n_data(df, value_col, n=10, ascending=False):
    """ìƒìœ„ Nê°œ ë°ì´í„° ì¶”ì¶œ"""
    return df.nlargest(n, value_col) if not ascending else df.nsmallest(n, value_col)

def calculate_percentiles(df, column):
    """ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°"""
    return {
        'min': df[column].min(),
        'q25': df[column].quantile(0.25),
        'median': df[column].median(),
        'q75': df[column].quantile(0.75),
        'max': df[column].max(),
        'mean': df[column].mean(),
        'std': df[column].std()
    }

def create_age_groups(ages, bins=None, labels=None):
    """ì—°ë ¹ ê·¸ë£¹ ìƒì„±"""
    if bins is None:
        bins = [0, 18, 35, 50, 65, 100]
    if labels is None:
        labels = ['18ì„¸ ë¯¸ë§Œ', '18-34ì„¸', '35-49ì„¸', '50-64ì„¸', '65ì„¸ ì´ìƒ']
    
    return pd.cut(ages, bins=bins, labels=labels, right=False)

def normalize_column(df, column):
    """ì»¬ëŸ¼ ì •ê·œí™”"""
    min_val = df[column].min()
    max_val = df[column].max()
    
    if max_val == min_val:
        return pd.Series([0.5] * len(df))
    
    return (df[column] - min_val) / (max_val - min_val)

def standardize_column(df, column):
    """ì»¬ëŸ¼ í‘œì¤€í™”"""
    mean_val = df[column].mean()
    std_val = df[column].std()
    
    if std_val == 0:
        return pd.Series([0] * len(df))
    
    return (df[column] - mean_val) / std_val

def detect_outliers(df, column, method='iqr'):
    """ì´ìƒì¹˜ íƒì§€"""
    if method == 'iqr':
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        return df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    
    elif method == 'zscore':
        z_scores = np.abs(standardize_column(df, column))
        return df[z_scores > 3]
    
    return pd.DataFrame()

def create_time_series_features(df, date_column):
    """ì‹œê³„ì—´ íŠ¹ì„± ìƒì„±"""
    df = df.copy()
    df[date_column] = pd.to_datetime(df[date_column])
    
    df['year'] = df[date_column].dt.year
    df['month'] = df[date_column].dt.month
    df['day'] = df[date_column].dt.day
    df['weekday'] = df[date_column].dt.weekday
    df['quarter'] = df[date_column].dt.quarter
    df['is_weekend'] = df['weekday'].isin([5, 6])
    
    return df

def calculate_moving_average(df, column, window=7):
    """ì´ë™í‰ê·  ê³„ì‚°"""
    return df[column].rolling(window=window).mean()

def calculate_correlation_matrix(df, numeric_columns):
    """ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°"""
    return df[numeric_columns].corr()

def get_memory_usage(df):
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸"""
    return df.memory_usage(deep=True).sum() / 1024 / 1024  # MB

def optimize_dataframe(df):
    """ë°ì´í„°í”„ë ˆì„ ìµœì í™”"""
    optimized_df = df.copy()
    
    # ì •ìˆ˜í˜• ìµœì í™”
    for column in optimized_df.select_dtypes(include=['int']).columns:
        col_min = optimized_df[column].min()
        col_max = optimized_df[column].max()
        
        if col_min >= -128 and col_max <= 127:
            optimized_df[column] = optimized_df[column].astype('int8')
        elif col_min >= -32768 and col_max <= 32767:
            optimized_df[column] = optimized_df[column].astype('int16')
        elif col_min >= -2147483648 and col_max <= 2147483647:
            optimized_df[column] = optimized_df[column].astype('int32')
    
    # ì‹¤ìˆ˜í˜• ìµœì í™”
    for column in optimized_df.select_dtypes(include=['float']).columns:
        optimized_df[column] = pd.to_numeric(optimized_df[column], downcast='float')
    
    # ë¬¸ìì—´ ìµœì í™”
    for column in optimized_df.select_dtypes(include=['object']).columns:
        num_unique_values = len(optimized_df[column].unique())
        num_total_values = len(optimized_df[column])
        
        if num_unique_values / num_total_values < 0.5:
            optimized_df[column] = optimized_df[column].astype('category')
    
    return optimized_df

def create_summary_statistics(df, numeric_columns):
    """ìš”ì•½ í†µê³„ ìƒì„±"""
    summary = df[numeric_columns].describe()
    
    # ì¶”ê°€ í†µê³„ ì •ë³´
    summary.loc['missing'] = df[numeric_columns].isnull().sum()
    summary.loc['missing_pct'] = (df[numeric_columns].isnull().sum() / len(df)) * 100
    
    return summary

def validate_data_quality(df):
    """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
    quality_report = {
        'total_rows': len(df),
        'total_columns': len(df.columns),
        'duplicate_rows': df.duplicated().sum(),
        'missing_values': df.isnull().sum().sum(),
        'memory_usage_mb': get_memory_usage(df)
    }
    
    # ì»¬ëŸ¼ë³„ ëˆ„ë½ê°’ ë¹„ìœ¨
    missing_pct = (df.isnull().sum() / len(df)) * 100
    quality_report['high_missing_columns'] = missing_pct[missing_pct > 50].index.tolist()
    
    return quality_report

def create_data_profile(df):
    """ë°ì´í„° í”„ë¡œíŒŒì¼ ìƒì„±"""
    profile = {
        'shape': df.shape,
        'columns': df.columns.tolist(),
        'dtypes': df.dtypes.to_dict(),
        'numeric_columns': df.select_dtypes(include=[np.number]).columns.tolist(),
        'categorical_columns': df.select_dtypes(include=['object', 'category']).columns.tolist(),
        'date_columns': df.select_dtypes(include=['datetime']).columns.tolist(),
        'quality_report': validate_data_quality(df)
    }
    
    return profile

def export_data_profile(df, filename):
    """ë°ì´í„° í”„ë¡œíŒŒì¼ ë‚´ë³´ë‚´ê¸°"""
    profile = create_data_profile(df)
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(profile, f, ensure_ascii=False, indent=2, default=str)
    
    return filename

def load_config(config_path):
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return {}
    except json.JSONDecodeError:
        st.error("ì„¤ì • íŒŒì¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return {}

def save_config(config, config_path):
    """ì„¤ì • íŒŒì¼ ì €ì¥"""
    try:
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        st.error(f"ì„¤ì • ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        return False

def create_backup(file_path):
    """íŒŒì¼ ë°±ì—… ìƒì„±"""
    if Path(file_path).exists():
        backup_path = f"{file_path}.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        try:
            import shutil
            shutil.copy2(file_path, backup_path)
            return backup_path
        except Exception as e:
            st.error(f"ë°±ì—… ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return None
    return None

def clean_text_data(text):
    """í…ìŠ¤íŠ¸ ë°ì´í„° ì •ì œ"""
    if pd.isna(text):
        return ""
    
    # ê¸°ë³¸ ì •ì œ
    text = str(text).strip()
    
    # ì—°ì†ëœ ê³µë°± ì œê±°
    text = " ".join(text.split())
    
    # íŠ¹ìˆ˜ë¬¸ì ì •ì œ (í•„ìš”ì— ë”°ë¼ ìˆ˜ì •)
    text = text.replace('\n', ' ').replace('\t', ' ').replace('\r', ' ')
    
    return text

def calculate_business_days(start_date, end_date):
    """ì˜ì—…ì¼ ê³„ì‚°"""
    start_date = pd.to_datetime(start_date)
    end_date = pd.to_datetime(end_date)
    
    return pd.bdate_range(start_date, end_date).shape[0]

def get_season(date):
    """ê³„ì ˆ êµ¬ë¶„"""
    month = pd.to_datetime(date).month
    
    if month in [12, 1, 2]:
        return 'ê²¨ìš¸'
    elif month in [3, 4, 5]:
        return 'ë´„'
    elif month in [6, 7, 8]:
        return 'ì—¬ë¦„'
    else:
        return 'ê°€ì„'

def create_cohort_analysis(df, customer_col, date_col, value_col=None):
    """ì½”í˜¸íŠ¸ ë¶„ì„ ë°ì´í„° ìƒì„±"""
    df = df.copy()
    df[date_col] = pd.to_datetime(df[date_col])
    
    # ì²« êµ¬ë§¤ ë‚ ì§œ
    df['first_purchase'] = df.groupby(customer_col)[date_col].transform('min')
    
    # ê¸°ê°„ ê³„ì‚°
    df['period'] = (df[date_col] - df['first_purchase']).dt.days
    
    # ì½”í˜¸íŠ¸ ê·¸ë£¹ ìƒì„±
    df['cohort_group'] = df['first_purchase'].dt.to_period('M')
    
    # ê¸°ê°„ ë²ˆí˜¸
    df['period_number'] = df['period'] // 30  # ì›” ë‹¨ìœ„
    
    # ì½”í˜¸íŠ¸ í…Œì´ë¸” ìƒì„±
    cohort_data = df.groupby(['cohort_group', 'period_number'])[customer_col].nunique().reset_index()
    cohort_table = cohort_data.pivot(index='cohort_group', 
                                    columns='period_number', 
                                    values=customer_col)
    
    # ë¹„ìœ¨ ê³„ì‚°
    cohort_sizes = df.groupby('cohort_group')[customer_col].nunique()
    cohort_percentages = cohort_table.divide(cohort_sizes, axis=0)
    
    return cohort_table, cohort_percentages

def check_data_freshness(file_path, max_age_hours=24):
    """ë°ì´í„° ì‹ ì„ ë„ í™•ì¸"""
    if not Path(file_path).exists():
        return False, "íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    
    file_modified = datetime.fromtimestamp(Path(file_path).stat().st_mtime)
    age = datetime.now() - file_modified
    
    if age.total_seconds() > max_age_hours * 3600:
        return False, f"ë°ì´í„°ê°€ {age.total_seconds()/3600:.1f}ì‹œê°„ ì „ì— ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤."
    
    return True, "ë°ì´í„°ê°€ ìµœì‹ ì…ë‹ˆë‹¤."

def log_user_action(action, details=None):
    """ì‚¬ìš©ì ì•¡ì…˜ ë¡œê·¸"""
    log_entry = {
        'timestamp': datetime.now().isoformat(),
        'action': action,
        'details': details or {}
    }
    
    # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
    if 'user_logs' not in st.session_state:
        st.session_state.user_logs = []
    
    st.session_state.user_logs.append(log_entry)
    
    # ë¡œê·¸ íŒŒì¼ì— ì €ì¥ (ì˜µì…˜)
    log_file = Path('logs/user_actions.log')
    log_file.parent.mkdir(exist_ok=True)
    
    try:
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(f"{json.dumps(log_entry, ensure_ascii=False)}\n")
    except Exception:
        pass  # ë¡œê·¸ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ

def get_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ ë°˜í™˜"""
    import platform
    import psutil
    
    return {
        'platform': platform.platform(),
        'python_version': platform.python_version(),
        'cpu_count': psutil.cpu_count(),
        'memory_total': psutil.virtual_memory().total // (1024**3),  # GB
        'memory_available': psutil.virtual_memory().available // (1024**3)  # GB
    }

def create_error_handler(func):
    """ì—ëŸ¬ í•¸ë“¤ëŸ¬ ë°ì½”ë ˆì´í„°"""
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            log_user_action('error', {'function': func.__name__, 'error': str(e)})
            return None
    return wrapper